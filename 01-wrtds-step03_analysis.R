## ---------------------------------------------- ##
           # WRTDS Centralized Workflow
## ---------------------------------------------- ##
# WRTDS = Weighted Regressions on Time, Discharge, and Season
## Nick J Lyon

## ---------------------------------------------- ##
                # Housekeeping ----
## ---------------------------------------------- ##
# Load libraries
# install.packages("librarian")
librarian::shelf(tidyverse, googledrive, lubridate, EGRET, EGRETci, lter/HERON, supportR)

# Clear environment
rm(list = ls())

# If working on server, need to specify correct path
(path <- scicomptools::wd_loc(local = FALSE, remote_path = file.path('/', "home", "shares", "lter-si", "WRTDS")))

# Make sure necessary folders exist
dir.create(path = file.path(path, "WRTDS Inputs"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Temporary Files"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Loop Diagnostic_Feb2024"), showWarnings = F)
dir.create(path = file.path(path, "WRTDS Outputs_Feb2024"), showWarnings = F)

# Identify CSVs generated by 'step-2' script
input_ids <- 
  googledrive::drive_ls(googledrive::as_id("https://drive.google.com/drive/u/0/folders/1QEofxLdbWWLwkOTzNRhI6aorg7-2S3JE"))

# Download them locally
purrr::walk2(.x = input_ids$name, .y = input_ids$id,
             .f = ~ googledrive::drive_download(file = .y, overwrite = T,
                                                path = file.path(path, "WRTDS Inputs_Feb2024", .x)))

# Read in CSVs generated by 'step-2' script
discharge <- read.csv(file.path(path, "WRTDS Inputs_Feb2024", "WRTDS-input_discharge.csv"))
chemistry <- read.csv(file.path(path, "WRTDS Inputs_Feb2024", "WRTDS-input_chemistry.csv"))
information <- read.csv(file.path(path, "WRTDS Inputs_Feb2024", "WRTDS-input_information.csv"))
crop <- read.csv(file.path(path,"WRTDS Inputs_Feb2024","WRTDS-data_cropping.csv"))

# format cropping object to modify blankTime columns for running WRTDS
crop_v1 <- crop |> 
  dplyr::mutate(Stream_ID = paste0(LTER, "__", Site),
                .before = dplyr::everything()) |> 
  # drop unwanted columns
  dplyr::select(-dplyr::ends_with("_Than"),starts_with("X."),-LTER,-Site) %>% 
  dplyr::select(Stream_ID,variable,BlankTime_Start,BlankTime_End) |> 
  # drop non-unique rows
  dplyr::distinct() %>% 
  # drop uncropped streams
  dplyr::filter(!(BlankTime_Start == "NA" & BlankTime_End == "NA")) %>% 
  # make years numeric; makes all "NAs" in original dataset into real NA
  dplyr::mutate(BlankTime_Start = suppressWarnings(as.numeric(BlankTime_Start)), 
                BlankTime_End = suppressWarnings(as.numeric(BlankTime_End))) |> 
  # make the value into a date
  dplyr::mutate(BlankTime_Start_Date = as.Date(paste0(BlankTime_Start,"-01","-01")),
                BlankTime_End_Date = as.Date(paste0(BlankTime_End,"-01","-01")))

## ---------------------------------------------- ##
            # Diagnose Types of Sites ----
## ---------------------------------------------- ##
# Set period of absence rivers (NOT AN ERROR!)
pa5_5 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 5)
  "NWT__MARTINELLI_DSi", "NWT__MARTINELLI_NH4", 
  "NWT__MARTINELLI_NOx", "NWT__MARTINELLI_P")

pa5_3 <- c(
  # EGRET::setPA(eList = egret_list[_out], paStart = 5, paLong = 3)
  "NWT__SADDLE STREAM 007_DSi", "NWT__SADDLE STREAM 007_NH4", 
  "NWT__SADDLE STREAM 007_NOx", "NWT__SADDLE STREAM 007_P")

## ---------------------------------------------- ##
  # ERRORS ----
## --------------------------------------------- ##

fixed <- c('Krycklan__Site 1_NO3','Krycklan__Site 10_NO3', 'Krycklan__Site 13_NO3',
           'Krycklan__Site 14_NO3', 'Krycklan__Site 16_NO3','Krycklan__Site 4_NO3',
           'Krycklan__Site 5_NO3','Krycklan__Site 6_NO3',
           'Krycklan__Site _7_NO3','Krycklan__Site 9_NO3',# didn't run but all others had too few samples
           'MCM__Canada Stream at F1_P', 'MCM__Onyx River at Lake Vanda Weir_NH4', # also had a "0.0" concentration
           'MCM__Onyx River at Lake Vanda Weir_P', 'MCM__Onyx River at Lower Wright Weir_P',
           'MCM__Priscu Stream at B1_NH4', 'MCM__Von Guerard Stream at F6_NH4',
           'NIVA__BUSEDRA_NH4', 'NIVA__BUSEDRA_P','NIVA__FINEALT_NH4', 'NIVA__FINEALT_P',
           'NIVA__NOREVEF_NH4',"NIVA__NOREVEF_P", 'NIVA__STREORK_NH4', 'NIVA__STREORK_P',
           'NIVA__TELESKI_NH4',
           'NIVA__TELESKI_P', 'NIVA__VAGEOTR_NH4', 'NIVA__VAGEOTR_P', 'NIVA__VESENUM_NH4',
           "NIVA__AAGEVEG_DSi",
           'NIVA__VESENUM_P','GRO__Kolyma_P','GRO__Lena_P','GRO__Mackenzie_P','GRO__Yukon_P',
           'Canada__KICKING HORSE RIVER AT FIELD IN YOHO NATIONAL PARK_NH4')

# Error in runSurvReg(SampleCrossV$DecYear[i], SampleCrossV$LogQ[i], DecLow,  : 
# minNumUncen is greater than total number of samples
few_data_cens <- c('AND__GSWS07_NOx','NWT__ALBION_P', 'AND__GSWS06_NOx',
              'Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_NOx',
              'Australia__BARWON RIVER AT DANGAR BRIDGE WALGETT_P',
              'Canada__BEAVER RIVER ABOVE HIGHWAY 1 IN GLACIER NATIONAL PARK_NH4', # only three obs?
              "Catalina Jemez__MG_WEIR_NH4",'Catalina Jemez__OR_low_NH4',
              'HBR__ws1_P','HBR__ws2_P','HBR__ws5_P','HBR__ws3_P',
              'HBR__ws4_P','HBR__ws6_P','HBR__ws7_P', 'HBR__ws8_P','HBR__ws9_P',
              'HYBAM__Nazareth_DSi',
              'NWT__SADDLE STREAM 007_P','USGS__ANDREWS CREEK_NH4','USGS__ANDREWS CREEK_P',
              "USGS__APALACHICOLA RIVER_P",'USGS__Arkansas River at Murray Dam_NH4',
              'USGS__Biscuit Brook_NOx','USGS__Biscuit Brook_P',
              'USGS__CANAJOHARIE CREEK_P','USGS__Dismal River_NH4',
              'USGS__EAGLE RIVER AT AVON_NH4','USGS__EAGLE RIVER AT RED CLIFF_NH4',
              'USGS__EAGLE RIVER AT RED CLIFF_P','USGS__EAGLE RIVER NEAR MINTURN_NH4',
              'USGS__EAGLE RIVER NEAR MINTURN_P','USGS__Flat Brook_P', 'USGS__Frio River_P',
              'USGS__GORE CREEK UPPER STATION_NH4','USGS__GORE CREEK UPPER STATION_P',
              'USGS__GREEN RIVER_NH4','USGS__GREEN RIVER_P,"USGS__HILLABAHATCHEE CREEK_NH4",
              "USGS__HILLABAHATCHEE CREEK_P','USGS__LITTLE RIVER_NH4','USGS__LITTLE RIVER_P',
              'USGS__McDonalds Branch_P','USGS__MERCED R_P','USGS__Merced River_P',
              'USGS__PINE CREEK_NH4','USGS__PINE CREEK_P','USGS__POPPLE RIVER_P',
              'USGS__ROARING FORK_P','USGS__SOPCHOPPY RIVER_NOx',
              'USGS__YAMPA RIVER AT STEAMBOAT SPRINGS_NH4',
              'HYBAM__Atalaya Aval_NO3','HYBAM__Borja_NO3',
              'HYBAM__Ciudad Bolivar_NO3',
              'HYBAM__Manacapuru_NOx','HYBAM__Manacapuru_P','HYBAM__Nazareth_NO3',
              'NWT__MARTINELLI_P','UK__DON AT DONCASTER_DSi','UK__DON AT SHEFFIELD HADFIELDS_DSi',
              'USGS__GREEN RIVER_P','USGS__HILLABAHATCHEE CREEK_NH4','USGS__HILLABAHATCHEE CREEK_P',
              'USGS__SOPE CREEK_P','USGS__SOUTH PLATTE_NH4','USGS__SOUTH PLATTE_NOx',
              'USGS__SOUTH PLATTE_P','USGS__ST. LAWRENCE_P','USGS__Vallecito Creek_P',
              'USGS__YUKON RIVER_P','Walker Branch__WALK_P',
              'Canada__KICKING HORSE RIVER AT FIELD IN YOHO NATIONAL PARK_NH4',
              'Krycklan__Site 1_NO3','Krycklan__Site 10_NO3', 'Krycklan__Site 13_NO3',
              'Krycklan__Site 14_NO3', 'Krycklan__Site 16_NO3','Krycklan__Site 4_NO3',
              'Krycklan__Site 5_NO3','Krycklan__Site 6_NO3',
              'Krycklan__Site _7_NO3','Krycklan__Site 9_NO3',
              'MCM__Onyx River at Lake Vanda Weir_NH4',
              'MCM__Onyx River at Lake Vanda Weir_P','MCM__Onyx River at Lower Wright Weir_P','MCM__Priscu Stream at B1_NH4')

# Error in runSurvReg(estPtYear, estPtLogQ, DecLow, DecHigh, localSample,  : 
# minNumObs is greater than total number of samples
few_data_obs <- c("MD__Barr Creek_DSi", 'MD__Barr Creek_NOx', 
                  'MD__Barr Creek_P',# issue in "Data cropping spreadsheet" cuts off everything before 2015
              'Sagehen__Sagehen_NH4') 

## Error in setupYears(paStart = paStart, paLong = paLong, localDaily = localDaily) : 
## Daily dataframe cannot have gaps in the data.
# missing data in "Daily"; zero concentration
missing_q <- c() 

# Set of problem rivers to drop from the loop (for reasons not specified above)
bad_rivers <- c(
  # River names with slashes cause a file path issue later on
  "UK__EDEN AT PENSHURST / VEXOUR BRIDGE_DSi", 
  "UK__MEDWAY AT TESTON / EAST FARLEIGH_DSi")

#Error in if (!(localUnits %in% allCaps)) { : the condition has length > 1
odd_ones <- c('USGS__Wild River_DSi','USGS__Wild River_NH4',
              'USGS__Wild River_NO3','USGS__Wild River_NOx','USGS__Wild River_P')

crash_rivers <- c()

# Identify all rivers that aren't in the broken data vectors
good_rivers <- setdiff(x = unique(chemistry$Stream_Element_ID),
                       y = unique(c(few_data_cens,few_data_obs, odd_ones, bad_rivers, crash_rivers)))
## Note this includes weird rivers that need special treatment and those that don't

## ---------------------------------------------- ##
              # Analysis Workflow ----
## ---------------------------------------------- ##

# Vector for storing problem rivers identified in latest run of WRTDS
new_bads <- c()
              
skipped <- c()

# Set of rivers we've already run the workflow for
done_rivers <- data.frame("file" = dir(path = file.path(path, "WRTDS Loop Diagnostic_Feb2024"))) %>%
  # Drop the file suffix part of the file name 
  dplyr::mutate(river = gsub(pattern = "\\_Loop\\_Diagnostic.csv", replacement = "", x = file))

# Identify particular subset to run
rivers_swed_dat <- chemistry %>% 
  separate_wider_delim(cols=Stream_ID,names=c("LTER","Stream_Name"),delim ="__",
                       too_many="debug") %>% 
  select(LTER,Stream_Element_ID) %>% 
  filter(LTER == "Swedish Goverment")

rivers_swed <- unique(rivers_swed_dat$Stream_Element_ID)

#
rivers_aus_dat <- chemistry %>% 
  separate_wider_delim(cols=Stream_ID,names=c("LTER","Stream_Name"),delim ="__",
                       too_many="debug") %>% 
  select(LTER,Stream_Element_ID) %>% 
  filter(LTER == "Australia")

rivers_aus <- unique(rivers_aus_dat$Stream_Element_ID)

## Final list of rivers to run
rivers_to_do <- rivers_aus

#rivers_to_do <- sort(setdiff(x = unique(good_rivers), 
  #                           y = c(unique(done_rivers$river), new_bads,skipped)))

# What are the next few that will be processed and how many total left?
rivers_to_do[1:5]; length(rivers_to_do)


# Loop across rivers and elements to run WRTDS workflow!
for(river in rivers_to_do){ # actual loop
  # for(river in rivers_to_do[1:3]){ # test of several rivers
  # for(river in "AND__GSMACK_DSi"){ # test a particular river
  
  # Loop - Set Up Steps ----
  
  # Identify corresponding Stream_ID
  stream_id <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(Stream_ID) %>%
    unique() %>%
    as.character()
  
  # Also element
  element <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    dplyr::select(variable) %>%
    unique() %>%
    as.character()
  
  # Subset chemistry
  river_chem <- chemistry %>%
    dplyr::filter(Stream_Element_ID == river) %>%
    # Drop unneeded columns
    dplyr::select(-Stream_Element_ID, -Stream_ID, -variable)
  
  # Subset discharge to correct river
  river_disc <- discharge %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    dplyr::select(Date, Q)
  
  # Create a common prefix for all outputs from this run of the loop
  out_prefix <- paste0(stream_id, "_", element, "_") 
  
  # Message completion of loop
  message("Processing begun for '", river, "'")
  
  # Grab start time for processing
  start <- Sys.time()
  
  # Information also subsetted to right river
  river_info <- information %>%
    dplyr::filter(Stream_ID == stream_id) %>%
    # Generate correct information for this element
    dplyr::mutate(constitAbbrev = element) %>%
    dplyr::mutate(paramShortName = dplyr::case_when(
      constitAbbrev == "DSi" ~ "Silicon",
      constitAbbrev == "NOx" ~ "Nitrate_NOx",
      constitAbbrev == "NO3" ~ "Nitrate_NO3",
      constitAbbrev == "P" ~ "Phosphorous",
      constitAbbrev == "NH4" ~ "Ammonium",
      constitAbbrev == "TP" ~ "Total_Phosphorous",
      constitAbbrev == "TN" ~ "Total_Nitrogen")) %>%
    # Create another needed column
    dplyr::mutate(staAbbrev = shortName) %>%
    # Drop stream ID now that subsetting is complete
    dplyr::select(-Stream_ID)
  
  # Save these as CSVs with generic names
  ## This means each iteration of the loop will overwrite them so this folder won't become gigantic
  write.csv(x = river_disc, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "discharge.csv"))
  write.csv(x = river_chem, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "chemistry.csv"))
  write.csv(x = river_info, row.names = F, na = "",
            file = file.path(path, "WRTDS Temporary Files", "information.csv"))
  
  # Then read them back in with EGRET's special acquisition functions
  egret_disc <- EGRET::readUserDaily(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "discharge.csv", qUnit = 2, verbose = F)
  egret_chem <- EGRET::readUserSample(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "chemistry.csv", verbose = F)
  egret_info <- EGRET::readUserInfo(filePath = file.path(path, "WRTDS Temporary Files"), fileName = "information.csv", interactive = F)
  
  # Loop - Define Initial Objects ----
  
  # Create a list of the discharge, chemistry, and information files
  egret_list <- EGRET::mergeReport(INFO = egret_info, Daily = egret_disc, Sample = egret_chem, verbose = F)
  
  # Run series
  egret_list_out <- EGRET::runSeries(eList = egret_list, windowSide = 11, minNumObs = 45, minNumUncen=45,
                                     verbose = F, windowS = 0.5)
  
  # Handle rivers that have blank time periods
if(stream_id %in% crop_v1$Stream_ID){
  egret_list_out <- EGRET::blankTime(eList=egret_list_out, 
                                     startBlank=unique(crop_v1[which(crop_v1$Stream_ID == stream_id),]$BlankTime_Start_Date), 
                                     endBlank = unique(crop_v1[which(crop_v1$Stream_ID == stream_id),]$BlankTime_End_Date)) }
  
  
## OLD ##
if(stream_id == "USGS__Mississippi River at Grafton"){
egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1981-10-01", 
                                       endBlank = "1982-09-29") }
  if(stream_id == "USGS__PICEANCE CREEK RYAN GULCH"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1998-10-01",
                                      endBlank = "1999-09-30") }
  if(stream_id == "USGS__YAMPA RIVER AT DEERLODGE PARK"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1994-10-01",
                                       endBlank = "1996-09-30") }
  if(stream_id == "USGS__YUKON RIVER"){
    egret_list_out <- EGRET::blankTime(eList = egret_list_out, startBlank = "1996-10-01",
                                       endBlank = "2001-09-29") }
  
  # Loop - Period of Absence Tweaks ----
  
  # Some rivers just need the period of absence tweaked
  ## McMurdo (12 to 2)
  if(stringr::str_sub(string = stream_id, start = 1, end = 3) == "MCM"){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 12, paLong = 2)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 12, paLong = 2) }
  ## 5 to 5
  if(river %in% unique(pa5_5)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 5)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 5) }
  ## 5 to 3
  if(river %in% unique(pa5_3)){
    egret_list <- EGRET::setPA(eList = egret_list, paStart = 5, paLong = 3)
    egret_list_out <- EGRET::setPA(eList = egret_list_out, paStart = 5, paLong = 3) }
  
  # Fit original model
  egret_estimation <- EGRET::modelEstimation(eList = egret_list, windowS = 0.5,
                                             minNumObs = 45, minNumUncen =45, verbose = F)
  
  # Fit WRTDS Kalman
  egret_kalman <- EGRET::WRTDSKalman(eList = egret_estimation, niter = 200, verbose = T)
  
  # Identify error statistics
  # egret_error <- EGRET::errorStats(eList = egret_estimation)
  egret_error <- EGRET::errorStats(eList = egret_kalman)
  
  # Save the error stats out
  write.csv(x = egret_error, file = file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "ErrorStats_WRTDS.csv")), row.names = F, na = "")
  
  # Calculate flux bias statistic
  # flux_bias <- EGRET::fluxBiasStat(localSample = EGRET::getSample(x = egret_estimation))
  flux_bias <- EGRET::fluxBiasStat(localSample = EGRET::getSample(x = egret_kalman))
  
  # Export that
  write.csv(x = flux_bias, file = file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "FluxBias_WRTDS.csv")), row.names = F, na = "")
  
  # Create PDF report of preliminary graphs
  HERON::egret_report(eList_estim = egret_estimation, eList_series = egret_list_out,
                      out_path = file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "WRTDS_GFN_output.pdf")))
  
  # Create annual averages
  egret_annual <- EGRET::tableResults(eList = egret_list_out)
  egret_annual_kalman <- EGRET::setupYears(localDaily = egret_kalman$Daily)
  
  # Export both as CSVs also
  write.csv(x = egret_annual, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "ResultsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_annual_kalman, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "ResultsTable_Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Identify monthly results
  egret_monthly <- EGRET::calculateMonthlyResults(eList = egret_list_out)
  egret_monthly_kalman <- EGRET::calculateMonthlyResults(eList = egret_kalman)
  
  # Export those
  write.csv(x = egret_monthly, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "Monthly_GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_monthly_kalman, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "Monthly_Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Extract daily chemical value from run
  egret_concentration <- egret_list_out$Daily
  egret_conc_kalman <- egret_kalman$Daily
  
  # Export those as well
  write.csv(x = egret_concentration, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "GFN_WRTDS.csv")), row.names = F, na = "")
  write.csv(x = egret_conc_kalman, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "Kalman_WRTDS.csv")), row.names = F, na = "")
  
  # Get flow normalized trends (flux and concentration)
  egret_flownorm <- HERON::egret_trends(eList_series = egret_list_out, flux_unit = 8)
  
  # Export it!
  write.csv(x = egret_flownorm, file.path(path, "WRTDS Outputs_Feb2024", paste0(out_prefix, "TrendsTable_GFN_WRTDS.csv")), row.names = F, na = "")
  
  # Grab the end processing time
  end <- Sys.time()
  
  date_of_run <- Sys.Date()
  
  # Combine timing into a dataframe
  loop_diagnostic <- data.frame("stream" = stream_id,
                                "chemical" = element,
                                "loop_date" = date_of_run,
                                "loop_start" = start,
                                "loop_end" = end)
  
  # Export this as well
  write.csv(x = loop_diagnostic, file.path(path, "WRTDS Loop Diagnostic_Feb2024", paste0(out_prefix, "Loop_Diagnostic.csv")), row.names = F, na = "")
  
  # Message completion of loop
  message("Processing complete for '", river, "'")
  
  # Remove all objects created inside loop
  ## This makes figuring out where the loop breaks *much* easier!
  rm(list = c("stream_id", "element", "river_chem", "river_disc", "out_prefix", 
              "start", "river_info", "egret_disc", "egret_chem", "egret_info", 
              "egret_list", "egret_list_out", "egret_estimation", "egret_error", 
              "flux_bias", "egret_annual", "egret_annual_kalman", 
              "egret_monthly", "egret_monthly_kalman", "egret_concentration", 
              "egret_conc_kalman", "egret_flownorm", "end", "loop_diagnostic"))
  
} # End loop

# Object creation order (for error diagnostics):
## 0. stream_id; element; river_chem; river_disc; out_prefix; start; river_info
## 1. egret_disc; egret_chem; egret_info -- `EGRET::readUser___`
## 2. egret_list; egret_list_out -- `EGRET::mergeReport` & `EGRET::runSeries`
## 3. egret_estimation -- `EGRET::modelEstimation`
## 4. egret_kalman -- `EGRET::WRTDSKalman`
## 5. egret_error -- `EGRET::errorStats`
## 6. flux_bias -- `EGRET::fluxBiasStat`
## 7A. egret_annual -- `EGRET::tableResults`
## 7B. egret_annual_kalman -- `EGRET::tableResults`
## 8. egret_monthly; egret_monthly_kalman -- `EGRET::calculateMonthlyResults`
## 9. egret_concentration; egret_conc_kalman
## 10. egret_flownorm -- `HERON::egret_trends`
## 11. end; loop_diagnostic

# End ----


### BASEMENT 



